Corpus 语料库；文集
lexicon: 词典，辞典
tokens (words)

———————————————
Word representation: word embedding / word vector

1. One-hot encoding vector
2. Frequency based Embedding
3. Prediction based Embedding

2. Frequency based Embedding
* Count Vector
* TF-IDF Vector
* Co-Occurrence Vector: context relavent
  Co-Occurrence and Context Window

3. Prediction based Embedding
   the meaning of a word can be inferred by the company (context) it keeps
   
   Convert all words w_i, ..., w_V to learn embedding matrix W, learning
   W = [ -- w_{e1} -- 
             ....
         -- w_{eV} --]_{V x N}
  
    
*1 CBOW(Continuous bag of words)
  predict target word given context words
  3-layer network:  activation (linear averages)
  
  ** Input: context word(s) 
  ** Output: target word
  ** Loss function: log loss (cross entropy loss)
  ** Word vector: The weight between the hidden layer and the output layer is taken as the word vector representation of the word

*2 Skip-gram model
   predict the context given a word
   3-layer network:  activation
 
  ** Input: target word 
  ** Output: context words
  ** Loss function: log loss
  ** Word vector: The weights between the input and the hidden layer are taken as the word vector representation after training

[refer]
https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec
https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3
https://www.cnblogs.com/pinard/p/7160330.html



